{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import numpy.random as random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = glob.glob('../../../../ICDAR-2019-SROIE/data/box/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list = []\n",
    "for text in question:\n",
    "    with open(text) as f:\n",
    "        txt = f.read()\n",
    "    txt_list.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_only_8(string):\n",
    "    comma_split_string = [elem for elem in string.split(',') if elem] \n",
    "    element_8_list = []\n",
    "    leftover = []\n",
    "    for idx, txt in enumerate(comma_split_string):\n",
    "        if idx < 8:\n",
    "            element_8_list.append(int(txt))\n",
    "        else:\n",
    "            leftover.append(txt)\n",
    "    element_8_list.append(','.join(leftover))\n",
    "    return element_8_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[split_only_8(row) for row in text.split('\\n')] for text in txt_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [pd.DataFrame(c, columns=['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4','y4','transcript']).dropna() for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = reduce(lambda a,b: pd.concat([a, b]), df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = list(df_long.transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = glob.glob('../../../../ICDAR-2019-SROIE/data/key/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = []\n",
    "for ans in answer:\n",
    "    with open(ans) as f:\n",
    "        json_list.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': 'INDAH GIFT & HOME DECO',\n",
       " 'date': '19/10/2018',\n",
       " 'address': '27, JALAN DEDAP 13, TAMAN JOHOR JAYA, 81100 JOHOR BAHRU, JOHOR.',\n",
       " 'total': '60.30'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df = pd.DataFrame(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_company = list(answer_df.company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_address = list(answer_df.address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_date = list(answer_df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_total = list(answer_df.total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_none = ' '.join(list(map(str, answer_df.company + answer_df.address + answer_df.date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "none = list(filter(lambda e: e not in not_none, text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_none = random.choice(none, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(626, 626, 626, 626, 500)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_company), len(set_date), len(set_address), len(set_total), len(set_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = {0:'company', 1:'date', 2:'address', 3:'None'}\n",
    "answer_inv = {value:key for key, value in answer.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_label_list = [(_, answer_inv['company']) for _ in set_company]\n",
    "date_label_list = [(_, answer_inv['date']) for _ in set_date]\n",
    "address_label_list = [(_, answer_inv['address']) for _ in set_address]\n",
    "none_label_list = [(_, answer_inv['None']) for _ in set_none]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_label = company_label_list + date_label_list + address_label_list + none_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import sys\n",
    "sys.path.append('../../../modules/database/')\n",
    "from db_util import *\n",
    "from db import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\hscho\\Desktop\\long8v/DB/'\n",
    "name = 'word_classificaton_data'\n",
    "description = 'data for word classification, no splitted'\n",
    "db_raw = create_data_db(path=path, name=name, description=description, data_class='RAW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2378it [00:00, 140280.09it/s]\n"
     ]
    }
   ],
   "source": [
    "cache = {}\n",
    "for index, value in tqdm(enumerate(corpus_label)):\n",
    "    cache = update_cache(cache, index=index, label=str(value[1]), image=None, text=str(value[0]), ref=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_cache_to_db(db_raw, db_raw.open_db(b'db_data'), cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------------------------------------------+\n",
      "| Key         | Value                                                    |\n",
      "+-------------+----------------------------------------------------------+\n",
      "| class       | RAW                                                      |\n",
      "| created     | 2020-09-09 14:21:27                                      |\n",
      "| db_data     | (database)                                               |\n",
      "| description | data for word classification, no splitted                |\n",
      "| name        | word_classificaton_data                                  |\n",
      "| note        |                                                          |\n",
      "| reference   | C:\\Users\\hscho\\Desktop\\long8v/DB/word_classificaton_data |\n",
      "| sep         | \t                                                        |\n",
      "| updated     | 2020-09-09 14:21:27                                      |\n",
      "+-------------+----------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print_env(db_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Environment at 0x20b29249f90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "register_db(db=db_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = list(zip(*corpus_label))[0]\n",
    "all_text_chr = list(map(lambda e: list(str(e)), all_text))\n",
    "tokenizer = preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(all_text_chr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = r'C:\\Users\\hscho\\Desktop\\long8v/DB/'\n",
    "with open('{}/tokenizer.json'.format(tokenizer_path), 'w') as f:\n",
    "    json.dump(tokenizer.to_json(), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sorie_kernel",
   "language": "python",
   "name": "sroie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
